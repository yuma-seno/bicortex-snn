# Bi-Cortex SNN (BC-SNN) Technical Specification

**Project Name:** Bi-Cortex SNN
**Target Platform:** Edge Devices (Microcontrollers, FPGA, Mobile)

---

## 1. コンセプト (Concept)

本モデルは、**単一のニューロン群（Single Neuron Substrate）の中に**「不変の知能（Thinking Cortex）」と「流動的な記憶（Memory Cortex）」を機能的に共存させ、それらを「インターフェース（Interface）」で滑らかに接続する、エッジデバイス向け自律学習アーキテクチャである。

### コア・フィロソフィー
1.  **Single Network, Heterogeneous Functions:** 物理的な層構造を持たず、ニューロンIDの区分のみで役割を分担する。
2.  **Semantic Resonance Gating (SRG):** 「意味的共鳴」をトリガーとする独自の学習則。思考野が意味（または報酬）を検知した瞬間のみ記憶野の可塑性を活性化させる。
3.  **Interface (IF):** TCとMCを接続する結合群。セットアップ時に学習・調整され、運用時は固定される。

---

## 2. アーキテクチャ (Architecture)

### 2.1 領域定義
物理的には $0 \dots N_{total}-1$ のIDを持つ単一のニューロン群である。

| 領域名 | ID範囲 | 役割 | パラメータ特性 |
| :--- | :--- | :--- | :--- |
| **思考野 (TC)** | $0 \dots N_{th}-1$ | 特徴抽出、評価(Critic)、反射 | **固定 (Fixed)** |
| **記憶野 (MC)** | $N_{th} \dots N_{total}-1$ | 文脈保持、連合(Actor) | **常時可塑 (Plastic)** |

**※ Interface:** TCとMCの間の結合として定義される（領域ではない）。

### 2.2 思考野 (TC) の詳細
* **入力処理:** CNN等を用いて高次元入力を処理する。
* **本能回路 (Instinct):**
    * **調節ニューロン (Modulator):** 味覚などの重要刺激に対し、脳全体へ可塑性制御信号（ドーパミン）を放出する。
    * **反射回路:** 学習の「きっかけ」となる即時行動を生成する。

---

## 3. 学習メカニズム (Learning Mechanism)

### 3.1 意味的共鳴ゲーティング (Semantic Resonance Gating / SRG)
本モデルは、誤差逆伝播法ではなく、**SRG** と呼ばれる局所学習則を用いる。これは「思考野の活動（意味）」と「記憶野の痕跡（文脈）」が共鳴した時のみ結合を強化する仕組みである。
報酬がない場合でも、思考野が強く反応する重要な概念であれば、その文脈は記憶野に保持される（教師なし連想記憶）。

### 3.2 実装: 3要素学習則
SRGを数理的に実現するために、以下の3要素を用いる。

1.  **Pre-synaptic Trace:** 過去の入力の痕跡。
2.  **Post-synaptic Activity:** 現在の出力。
3.  **Resonance Factor:** 思考野からの評価信号（ドーパミン）または活動レベル（ゲート信号）。

### 3.3 学習プロセス
1.  **Trace:** 外部刺激がInterface経由で入力され、記憶野に痕跡が残る。
2.  **Resonance:**
    * 思考野が特定の概念を認識して強く活動する（Meaning）、または報酬刺激によりModulatorが発火する（Reward）と、SRGが作動する。
    * 共鳴係数に基づき、痕跡を持つシナプスが強化される。
3.  **Extinction:** 共鳴がない結合は、自然忘却（Decay）によって消滅する。

---

## 4. 開発ロードマップ (Development Roadmap)

### Phase 1: コアロジック検証 (Proof of Concept)
Python/NumPyによる最小構成シミュレーション。
* **目的:** 基本原理（SRG, Dual Traces, Interface Learning）が機能するかの確認。
* **構成:**
    * TC: ダミーのルールベース入力（ベル入力、エサ入力）。
    * MC: 50~100ニューロンのLSM。
* **タスク:** 古典的条件付け（パブロフの犬）。ベル（予兆）からエサ（報酬）までの時間差を学習し、ベルだけで反応できるか検証。
* **Output:** `bicortex_poc_v1.py`

### Phase 2: 視覚エンコーディング統合
画像入力を扱えるようにし、空間認識能力を付与する。
* **目的:** 実践的な入力データ（画像）の処理。
* **構成:**
    * TC: **MobileNetV3 (Small)** などの軽量CNNをバックボーンに使用。その特徴マップをSNN電流に変換して入力。
    * MC: 200~500ニューロン規模へ拡張。
* **タスク:** 2Dグリッドワールドにおける、動的な障害物回避。
* **Output:** `bicortex_visual_nav.py`

### Phase 3: リアルタイム・アクション適応
動的に変化する環境での適応能力と堅牢性の検証。
* **目的:** ゲーム等の複雑な環境での実用性確認。
* **構成:** Phase 2のモデルを強化し、OpenAI Gym (Atari) や自作ゲーム環境に接続。
* **シナリオ:**
    * 「特定の色の背景の時だけ、敵の行動パターンが変わる」といった、文脈依存のルール変化に適応できるかテストする。
* **Output:** `bicortex_action_learner.py`

### Phase 4: [Experimental] Spiking LLM 統合実験
**本モデルの拡張可能性を探るための実験的フェーズ。**
* **目的:** 思考野(TC)のバックボーンを、CNNから**Spiking LLM (RWKV/Spikformer等)** に置き換えてみる。
* **仮説:** 言語モデルが持つ高度な世界解釈能力を「概念ニューロン」として利用することで、自然言語による指示や、複雑な論理的文脈を記憶野に保持させることが可能になるのではないか。
* **アプローチ:**
    * 小規模なSpiking LLM (例: RWKV-100M程度) をTC領域に配置。
    * 言語入力に対するLLMの内部発火パターンをMCへ流し込み、「言葉の意味」と「行動」の連想記憶を試みる。
* **Output:** `bicortex_spiking_llm_test.py`

---