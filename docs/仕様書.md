# Bi-Cortex SNN (BC-SNN) Technical Specification

**Project Name:** Bi-Cortex SNN
**Date:** 2026-01-13
**Target Platform:** Edge Devices (Microcontrollers, FPGA, Mobile)

---

## 1. コンセプト (Concept)

本モデルは、**「不変の知能（Thinking Cortex）」**と**「流動的な記憶（Memory Cortex）」**を単一のニューロン群の中に共存させた、エッジデバイス向けの軽量・リアルタイムSNNモデルである。

### コア・フィロソフィー
1.  **単一ネットワーク・異種機能:** 物理的な層（Layer）構造を持たず、ニューロンIDによる領域区分（Parcellation）のみで「推論」と「記憶」の役割を分担する。
2.  **意味的共鳴 (Semantic Resonance):** バックプロパゲーションを用いず、思考野が「意味」を検知した瞬間のみ局所的な可塑性を活性化させることで、超低コストな学習を実現する。
3.  **時間的因果性の克服:** リカレント構造と長時間トレース変数を組み合わせ、数秒〜数十秒のタイムラグがある事象間の因果関係を学習する。

---

## 2. ニューロン・アーキテクチャ (Architecture)

全ニューロン数 $N_{total}$ のフラットな配列を定義し、ID範囲によって機能領域を区分する。

### 2.1 領域定義 (Parcellation)

| 領域名 | ID範囲 | 役割 | パラメータ特性 |
| :--- | :--- | :--- | :--- |
| **思考野 (Thinking Cortex / TC)** | $0 \dots N_{th}-1$ | 特徴抽出、概念認識、反射的行動 | **固定 (Fixed)** |
| **記憶野 (Memory Cortex / MC)** | $N_{th} \dots N_{total}-1$ | 文脈保持、時系列パターンの統合 | **動的 (Plastic)** |

### 2.2 思考野 (TC) の内部構造
TCは事前学習済みの知識構造を持ち、以下の3層で構成される。

1.  **入力層 (Input):** 感覚センサーからの入力をスパイクとして受け取る。
2.  **中間層 (Hidden):** 入力特徴の変換や抽象化を行う（スパース固定結合）。
3.  **運動層 (Motor):** 最終的なアクション信号を出力する。

### 2.3 記憶野 (MC) の構成
* **リザーバ構造:** ニューロン同士がランダムかつスパース（接続率10%程度）に相互結合されたリカレント回路。
* **E/Iバランス (Dale's Law):**
    * 興奮性 (Excitatory): 80%
    * 抑制性 (Inhibitory): 20%
    * ※抑制性ニューロンの導入により、記憶活動の爆発を防ぎ「カオスの縁 (Edge of Chaos)」状態を維持する。

---

## 3. 結合トポロジーと情報の流れ (Connectivity & Flow)

### 3.1 固定結合 (Fixed Connections) - 事前知識
* **Input $\to$ Hidden $\to$ Motor:** 反射的な基本行動や特徴抽出を行う固定回路。
* **TC (Input/Hidden) $\to$ MC (Context Injection):** 思考野の活動パターンを記憶野へブロードキャストし、現在のコンテキストを注入する。

### 3.2 可塑的結合 (Plastic Connections) - 短期記憶
**意味的共鳴ゲーティング**によってリアルタイムに重みが更新される。
* **$MC \to MC$ (Recurrent):** 入力されたコンテキストを時間的に保持・変形し、短期記憶（残響）を形成する。
* **$MC \to TC_{Hidden/Motor}$ (Interface Synapses):**
    * 記憶野の残響パターンを、思考野の処理や行動にフィードバックする。
    * これにより、「今の視覚情報」だけでなく、「過去の文脈」に基づいて行動を変化させる（連想記憶）。

---

## 4. ダイナミクスと数式 (Dynamics & Math)

### 4.1 ニューロンモデル (LIF)
計算効率を最優先し、Leaky Integrate-and-Fire モデルを採用。

$$v_i(t) = v_i(t-1) \cdot \alpha_{decay} + I_{input}(t) + \sum_{j} w_{ij} S_j(t-1)$$

* 発火条件: $v_i \ge v_{th}$ ならば $S_i(t)=1, v_i(t)=0$

### 4.2 2つのトレース変数 (Dual Traces)
1.  **即時トレース ($x_{fast}$):** $\tau_{fast} \approx 20ms$。通常のシナプス伝達用。
2.  **適格性トレース ($e_{slow}$):** $\tau_{slow} \approx 2.0s \sim 5.0s$。因果関係学習用。

---

## 5. 学習アルゴリズム: SRG (Soft-bound Hebbian)

### 5.1 ゲート信号 $G(t)$
思考野の活動量の**移動平均 (Moving Average)** が、一定の割合を超えた場合にゲートを開く。
$$Activity_{MA}(t) = (1-\alpha) \cdot Activity_{MA}(t-1) + \alpha \cdot \sum S_{TC}(t)$$
$$G(t) = 1 \quad \text{if} \quad Activity_{MA}(t) \ge \theta_{ratio} \cdot N_{TC} \quad \text{else} \quad 0$$

### 5.2 重み更新則 (Soft-bound)
重みの飽和と反転（Dale's Law違反）を防ぐため、現在の重み値に応じた更新を行う。

$$\Delta w_{ij}(t) = (W_{limit} - w_{ij}) \cdot \eta \cdot G(t) \cdot e_{slow, j}(t)$$

* **Global Weight Decay:** 学習の安定性を保つため、可塑的結合全体に対して毎ステップ微小な減衰（忘却）を適用する。

---

## 6. 開発ロードマップ (Development Roadmap)

(以下、既存の内容と同様のため省略)