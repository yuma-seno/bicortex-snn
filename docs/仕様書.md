# Bi-Cortex SNN (BC-SNN) Technical Specification

**Project Name:** Bi-Cortex SNN
**Target Platform:** Edge Devices (Microcontrollers, FPGA, Mobile)

---

## 1. コンセプト (Concept)

本モデルは、**単一のニューロン群（Single Neuron Substrate）の中に**「不変の知能（Thinking Cortex）」と「流動的な記憶（Memory Cortex）」を機能的に共存させ、それらを「インターフェース（Interface）」で滑らかに接続する、エッジデバイス向け自律学習アーキテクチャである。

### コア・フィロソフィー
1.  **Single Network, Heterogeneous Functions:** 物理的な層（Layer）構造やモジュール分割を持たず、フラットなニューロン配列上の領域区分（Parcellation）のみで役割を分担する。
2.  **Thinking Cortex (TC):** 事前学習済みの固定的な知能。特徴抽出、本能的評価、反射行動を担う。CNNやLLMなどの既存モデルを利用。
3.  **Interface (IF):** **TCとMCを接続するシナプス結合群。** 領域としては存在しない。セットアップ時に学習・調整され、運用時は固定される。
4.  **Memory Cortex (MC):** 経験によって配線が変わる可塑的な記憶。短期記憶の保持と、感覚・行動の連合学習を担う。運用時にリアルタイム学習を行う唯一の領域。

---

## 2. アーキテクチャ (Architecture)

### 2.1 領域定義
物理的には $0 \dots N_{total}-1$ のIDを持つ単一のニューロン群であり、IDの範囲によって以下の役割が割り当てられる。
**Interfaceはニューロン領域としては存在せず、TCとMCの間の結合として定義される。**

| 領域名 | ID範囲 | 役割 | パラメータ特性 |
| :--- | :--- | :--- | :--- |
| **思考野 (TC)** | $0 \dots N_{th}-1$ | 特徴抽出、評価(Critic)、反射 | **固定 (Fixed)** |
| **記憶野 (MC)** | $N_{th} \dots N_{total}-1$ | 文脈保持、連合(Actor) | **常時可塑 (Plastic)** |

### 2.2 思考野 (TC) の詳細
* **入力処理:** CNN等を用いて高次元入力を処理しやすい特徴ベクトルに変換する。
* **本能回路 (Instinct):**
    * **報酬ニューロン:** 有利な状態（味覚など）で発火し、全脳に強化信号を送る。
    * **反射回路:** 特定の入力に対して即座に行動（運動ニューロン発火）を起こすハードウェア回路。これが学習の「きっかけ（教師信号）」となる。

### 2.3 記憶野 (MC) の詳細
* **汎用連合野:** 特定のモダリティに特化せず、Interface（TCとの結合）から送られてくる信号を受け入れるリザーバ層。
* **短期記憶:** リカレント結合やニューロンの時定数により、入力情報を一時的に保持（Trace）する。

---

## 3. 学習メカニズム (Learning Mechanism)

### 3.1 3要素学習則 (Trace-based 3-Factor Rule)
シナプス結合 $\Delta w_{ij}$ の更新は以下の要素で決定される。

1.  **Pre-synaptic Trace ($Pre_{slow}$):** 入力側ニューロンが過去に発火していたか（痕跡）。
2.  **Post-synaptic Activity ($Post$):** 出力側ニューロンが発火したか。
3.  **Global Reward ($R$):** 思考野から供給されるドーパミン信号。

### 3.2 学習フェーズ (Learning Phases)
1.  **Phase 1 (Thinking Pre-training):** 思考野単体をオフラインで学習（Backprop等）。
2.  **Phase 2 (Interface Calibration):** 思考野と記憶野を接続する**結合重み**を調整し、信号伝達を最適化する。
3.  **Phase 3 (Online Learning):** 実運用フェーズ。TCとInterface結合は固定され、MC内部の結合のみがリアルタイムに学習する。

### 3.3 運用時のプロセス (Online Loop)
1.  **経験 (Experience):** 外部刺激がInterface結合を経由して入力され、記憶野に痕跡が残る。
2.  **強化 (Reinforcement):** 報酬が発生すると、記憶野の痕跡と運動野の発火が重なるシナプスが強化される。
3.  **淘汰 (Extinction):** 報酬が得られない行動（誤発火）に関連するシナプスは、自然忘却（Decay）によって消滅する。

---

## 4. 数理モデルと実装変数対応 (Mathematical Model)

### 4.1 ニューロンモデル (LIF)
$$v_i(t) = v_i(t-1) \cdot \alpha + I_{ext}(t) + \sum_{j} w_{ij} x_{fast, j}(t-1)$$

### 4.2 変数対応表 (`src/core/engine.py`)

| 数式記号 | 変数名 | 説明 | 設定値(例) |
| :--- | :--- | :--- | :--- |
| $v_i(t)$ | `self.v` | 膜電位 | 初期値 0.0 |
| $\alpha$ | `self.alpha` | 電圧減衰係数 | $\tau_m=20ms$ 相当 |
| $v_{th}$ | `self.v_base` | 発火閾値 | 1.0 ~ 2.0 |
| $x_{fast}$ | `self.x_fast` | 即時トレース (PSC) | $\tau_{fast} \approx 5ms$ |
| $e_{slow}$ | `self.e_slow` | 適格性トレース (学習用) | $\tau_{slow} \approx 1000ms$ |
| $w_{ij}$ | `self.W` | シナプス結合行列 | $N_{total} \times N_{total}$ |

---

## 5. 開発ロードマップ (Development Roadmap)

### Phase 1: コアロジック検証 (Proof of Concept)
Python/NumPyによる最小構成シミュレーション。
* **目的:** 基本原理（SRG, Dual Traces, Interface Learning）が機能するかの確認。
* **構成:**
    * TC: ダミーのルールベース入力（ベル入力、エサ入力）。
    * MC: 50~100ニューロンのLSM。
* **タスク:** 古典的条件付け（パブロフの犬）。ベル（予兆）からエサ（報酬）までの時間差を学習し、ベルだけで反応できるか検証。
* **Output:** `bicortex_poc_v1.py`

### Phase 2: 視覚エンコーディング統合
画像入力を扱えるようにし、空間認識能力を付与する。
* **目的:** 実践的な入力データ（画像）の処理。
* **構成:**
    * TC: **MobileNetV3 (Small)** などの軽量CNNをバックボーンに使用。その特徴マップをSNN電流に変換して入力。
    * MC: 200~500ニューロン規模へ拡張。
* **タスク:** 2Dグリッドワールドにおける、動的な障害物回避。
* **Output:** `bicortex_visual_nav.py`

### Phase 3: リアルタイム・アクション適応
動的に変化する環境での適応能力と堅牢性の検証。
* **目的:** ゲーム等の複雑な環境での実用性確認。
* **構成:** Phase 2のモデルを強化し、OpenAI Gym (Atari) や自作ゲーム環境に接続。
* **シナリオ:**
    * 「特定の色の背景の時だけ、敵の行動パターンが変わる」といった、文脈依存のルール変化に適応できるかテストする。
* **Output:** `bicortex_action_learner.py`

### Phase 4: [Experimental] Spiking LLM 統合実験
**本モデルの拡張可能性を探るための実験的フェーズ。**
* **目的:** 思考野(TC)のバックボーンを、CNNから**Spiking LLM (RWKV/Spikformer等)** に置き換えてみる。
* **仮説:** 言語モデルが持つ高度な世界解釈能力を「概念ニューロン」として利用することで、自然言語による指示や、複雑な論理的文脈を記憶野に保持させることが可能になるのではないか。
* **アプローチ:**
    * 小規模なSpiking LLM (例: RWKV-100M程度) をTC領域に配置。
    * 言語入力に対するLLMの内部発火パターンをMCへ流し込み、「言葉の意味」と「行動」の連想記憶を試みる。
* **Output:** `bicortex_spiking_llm_test.py`

---